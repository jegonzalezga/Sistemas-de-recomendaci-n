{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIIA-4203 MODELOS AVANZADOS PARA ANÁLISIS DE DATOS II\n",
    "\n",
    "\n",
    "# Sistemas de Recomendación de filtrado colaborativo con SurPRISE\n",
    "\n",
    "## Actividad 13\n",
    "\n",
    "### Profesor: Camilo Franco (c.franco31@uniandes.edu.co)\n",
    "\n",
    "En este cuadernos estudiaremos distintos métodos de filtrado colaborativo, junto con algunas métricas de desempeño de los algoritmos de recomendación. En la actividad anterior vimos un primer método dentro de la familia de métodos de factores latentes (por factorización matricial) con preferencias implícitas. En este cuaderno vamos a explorar otra familia de métodos, los cuales exploran *vecindades* o relaciones entre items o entre usuarios, basados en los K-vecinos más cercanos. Vamos a estudiar la aplicación de estos métodos sobre preferencias explícitas de la mano de la biblioteca de Python SurPRISE (Simple Python Recommendation System Engine) http://surpriselib.com/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar los siguientes paqutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además vamos a usar `plotly`, una bilbioteca de Python que permite visualizar los datos bajo múltiples tipos de gráficos. Además cuenta con un amplio rango de casos de uso. Este paquete plotly.py está construido sobre la bilbioteca Plotly JavaScript (plotly.js), permitiendo crear visualizaciones interactivas (para visualización en Jupyter, que se pueden guardar como archivos HTML, o que también se pueden usar en aplicaciones de Python en la web usando Dash https://plotly.com/dash/).\n",
    "\n",
    "https://plotly.com/python/getting-started/?utm_source=mailchimp-jan-2015&utm_medium=email&utm_campaign=generalemail-jan2015&utm_term=bubble-chart\n",
    "\n",
    "Para su instalacion en Anaconda: $ conda install -c plotly plotly=4.7.0\n",
    "\n",
    "En GoogleColab se puede importar directamente: https://colab.research.google.com/notebooks/charts.ipynb#scrollTo=8YCVGqZkJJxT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente vamos a usar SurPRISE. \n",
    "\n",
    "Para su instalación, debe ejecutar la siguiente linea desde la terminal (conda prompt): \n",
    "\n",
    "`conda install -c conda-forge scikit-surprise`\n",
    "\n",
    "Si tiene problemas, puede chequear la version de scipy en su computador. Debe ser una version reciente.\n",
    "Para ello ejecute: pip install \"scipy>=1.0\" \n",
    " ..y para instalar la bilbioteca: pip install scikit-surprise\n",
    " \n",
    "*Para ejecutar SurPRISE en GoogleColab,*\n",
    "- !pip uninstall -y scipy\n",
    "- !pip install scipy==1.0.0\n",
    "- !pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "\n",
    "from surprise import Reader\n",
    "from surprise import SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import LeaveOneOut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Visualización de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajemos con nuestros datos de peliculas IMDB (https://www.imdb.com/) con los ratings que algunos usuarios le han dado a algunas peliculas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis = pd.read_csv('movies_metadata.csv', low_memory=False)\n",
    "\n",
    "df = pd.read_csv('ratings_small.csv')\n",
    "df.drop(['timestamp'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos la distribucion de los ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_notebook_mode()#(connected=True)\n",
    "\n",
    "data = df['rating'].value_counts().sort_index(ascending=False)\n",
    "trace = go.Bar(x = data.index,\n",
    "               text = ['{:.1f} %'.format(val) for val in (data.values / df.shape[0] * 100)],\n",
    "               textposition = 'auto',\n",
    "               textfont = dict(color = '#000000'),\n",
    "               y = data.values,\n",
    "               )\n",
    "# Creamos el diseño\n",
    "layout = dict(title = 'Distribucion de los {} ratings'.format(df.shape[0]),\n",
    "              xaxis = dict(title = 'Rating'),\n",
    "              yaxis = dict(title = 'Frecuencias'))\n",
    "# Graficamos\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)\n",
    "#plot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la distribucion de ratings por pelicula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conteo de ratings por pelicula\n",
    "data = df.groupby('movieId')['rating'].count()\n",
    "\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 100,\n",
    "                                  size = 2))\n",
    "# Diseño\n",
    "layout = go.Layout(title = 'Distribucion del numero de ratings por pelicula (hasta 100 peliculas)',\n",
    "                   xaxis = dict(title = 'Ratings por pelicula'),\n",
    "                   yaxis = dict(title = 'Peliculas'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Grafica\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la mayoría de peliculas reciben un solo rating! Y la que más, recibe 341 ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('movieId')['rating'].count().reset_index().sort_values('rating', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, la pelicula con ID=296 recibe 324 ratings de los usuarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelis[pelis.id=='296']['original_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la distribucion de ratings por usuario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Conteo de ratings por usuario\n",
    "data = df.groupby('userId')['rating'].count().clip(upper=200)\n",
    "\n",
    "trace = go.Histogram(x = data.values,\n",
    "                     name = 'Ratings',\n",
    "                     xbins = dict(start = 0,\n",
    "                                  end = 200,\n",
    "                                  size = 2))\n",
    "# Diseño\n",
    "layout = go.Layout(title = 'Distribucion del numero de ratings por usuario (hasta 200 usuarios)',\n",
    "                   xaxis = dict(title = 'Ratings por usuario'),\n",
    "                   yaxis = dict(title = 'Usuarios'),\n",
    "                   bargap = 0.2)\n",
    "\n",
    "# Grafica\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igualmente, vemos que la mayoría de usuarios dan solo unos pocos ratings!\n",
    "\n",
    "Veamos los usuarios que dan un mayor numero de ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('userId')['rating'].count().reset_index().sort_values('rating', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, el usuario que más ha calificado películas ha calificado 2391 peliculas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.userId==547].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entonces observamos que muchos usuarios han calificado menos de 30 peliculas, y que muchas peliculas han recibido menos de 5 ratings. \n",
    "\n",
    "Con el fin de reducir el coste computacional de nuestros calculos y poder ejecutar más fácilmente el código que sigue vamos a fitrar las peliculas menos calificadas y los usuarios menos activos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_p_ratings = 5\n",
    "filter_p = df['movieId'].value_counts() > min_p_ratings\n",
    "filter_p = filter_p[filter_p].index.tolist()\n",
    "\n",
    "min_u_ratings = 30\n",
    "filter_u = df['userId'].value_counts() > min_u_ratings\n",
    "filter_u = filter_u[filter_u].index.tolist()\n",
    "\n",
    "df_nuevo = df[(df['movieId'].isin(filter_p)) & (df['userId'].isin(filter_u))]\n",
    "print('Los datos originales tienen tamaño:\\t{}'.format(df.shape))\n",
    "print('Los nuevo datos tienen tamaño:\\t{}'.format(df_nuevo.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Métodos de filtrado colaborativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a implementar una batería de modelos que SurPRISE tiene programados para que nosotros los apliquemos facilmente. Estos modelos nos sirven como punto de partida para intentar mejorarlos desarrollando nuestros propios modelos.\n",
    "\n",
    "Primero debemos cargar nuestros datos. para ello, utilizamos la función `load_from_df()`, utilizando la función `Reader` y especificando la escala en la que se da el *Rating*. En este caso los ratings van del 0 al 5. \n",
    "\n",
    "Nuestros datos deben constar de 3 columnas, los ids de usuario, los ids de los items, y los ratings (en este orden). Cada observación contiene el rating correspondiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(0, 5))\n",
    "data = Dataset.load_from_df(df_nuevo[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montados en SurPRISE, podemos implementar los siguientes algoritmos (antes de entrar en la teoría ejecutemos la celda abajo pues la validación cruzada de todos los modelos toma tiempo): \n",
    "\n",
    "**Algoritmos base**\n",
    "\n",
    "- NormalPredictor\n",
    "\n",
    "Este algoritmo predice un rating aleatorio asumiendo que la muestra de entrenamiento proviene de una distribución Normal:\n",
    "\n",
    "$$\\hat r_{ui}\\sim Normal(\\hat \\mu, \\hat \\sigma)  $$\n",
    "\n",
    "donde \n",
    "\n",
    "$$ \\hat \\mu = \\frac{1}{|R_{entrenamiento}|}\\sum_{r_{ui} \\in R_{entrenamiento}} r_{ui}  $$\n",
    "\n",
    "$$ \\hat \\sigma= \\sqrt{\\sum_{r_{ui} \\in R_{entrenamiento}} \\frac{(r_{ui}-\\hat \\mu)^2}{|R_{entrenamiento}|}} $$\n",
    "\n",
    "\n",
    "- BaselineOnly\n",
    "\n",
    "El algoritmo obtiene su estimación a partir del rating medio y las desviaciones observadas para la pelicula $i$ y el usuario $u$:\n",
    "\n",
    "\n",
    "$$\\hat r_{ui}= \\mu + b_i + b_u$$\n",
    "\n",
    "donde $\\mu$ es el rating promedio de los datos de entrenamiento, $b_i$ que es el rating promedio del item $i$ menos $\\mu$ y $b_u$ que es el rating promedio del usuario $u$ menos $\\mu$.\n",
    "\n",
    "\n",
    "**Algoritmos de vecindades:**\n",
    "\n",
    "- KNNBasic\n",
    "\n",
    "El *KNNBasic* es un modelo que estima los ratings de acuerdo con los $k$ vecinos más cercanos, ya sea por usuario o por item, de acuerdo con:\n",
    "\n",
    "$$ \\hat r_{ui}=\\frac{\\sum_{v\\in N_i^k(u)}sim(u,v) \\cdot r_{vi}}{\\sum_{v\\in N_i^k(u)}sim(u,v)}  $$\n",
    "\n",
    "ó\n",
    "\n",
    "$$ \\hat r_{ui}=\\frac{\\sum_{j\\in N_u^k(i)}sim(i,j) \\cdot r_{uj}}{\\sum_{j\\in N_u^k(i)}sim(i,j)}  $$\n",
    "\n",
    "donde $sim$ es la función de similitud (https://surprise.readthedocs.io/en/stable/similarities.html#module-surprise.similarities)\n",
    "\n",
    "\n",
    "- KNNWithMeans\n",
    "\n",
    "Este algoritmo modifica el *KNNBasic* tomando además en cuenta los ratings promedios de los usuarios:\n",
    "\n",
    "$$ \\hat r_{ui}=\\mu_u + \\frac{\\sum_{v\\in N_i^k(u)}sim(u,v) \\cdot (r_{vi}-\\mu_v)}{\\sum_{v\\in N_i^k(u)}sim(u,v)}  $$\n",
    "\n",
    "ó\n",
    "\n",
    "$$ \\hat r_{ui}=\\mu_i + \\frac{\\sum_{j\\in N_u^k(i)}sim(i,j) \\cdot (r_{uj}-\\mu_j)}{\\sum_{j\\in N_u^k(i)}sim(i,j)}  $$\n",
    "\n",
    "\n",
    "- KNNWithZScore\n",
    "\n",
    "Este algoritmo toma en cuenta las similitudes y los ratings estandarizados:\n",
    "\n",
    "$$ \\hat r_{ui}=\\mu_u + \\sigma_u\\frac{\\sum_{v\\in N_i^k(u)}sim(u,v) \\cdot (r_{vi}-\\mu_v)/\\sigma_v}{\\sum_{v\\in N_i^k(u)}sim(u,v)}  $$\n",
    "\n",
    "ó\n",
    "\n",
    "$$ \\hat r_{ui}=\\mu_i + \\sigma_i \\frac{\\sum_{j\\in N_u^k(i)}sim(i,j) \\cdot (r_{uj}-\\mu_j)/\\sigma_j}{\\sum_{j\\in N_u^k(i)}sim(i,j)}  $$\n",
    "\n",
    "\n",
    "\n",
    "- KNNBaseline\n",
    "\n",
    "Este algoritmo toma en cuenta el rating medio y las desviaciones observadas para la pelicula $i$ y el usuario $u$:\n",
    "\n",
    "$$ \\hat r_{ui}=b_{ui} + \\frac{\\sum_{v\\in N_i^k(u)}sim(u,v) \\cdot (r_{vi}-b_{vi})}{\\sum_{v\\in N_i^k(u)}sim(u,v)}  $$\n",
    "\n",
    "ó\n",
    "\n",
    "$$ \\hat r_{ui}=b_{ui} + \\frac{\\sum_{j\\in N_u^k(i)}sim(i,j) \\cdot (r_{uj}-b_{uj})}{\\sum_{j\\in N_u^k(i)}sim(i,j)}  $$\n",
    "\n",
    "**Algoritmos de factores latentes:**\n",
    "\n",
    "- SVD\n",
    "\n",
    "Este algoritmo corresponde con la factorización de la matriz de ratings (visto en la actividad anterior):\n",
    "\n",
    "$$\\hat r_{ui}=q_{i}'p_{u} + \\mu + b_i + b_u$$\n",
    "\n",
    "- SVDpp\n",
    "\n",
    "Este algoritmo extiende el SVD al tomar en cuenta los ratings implícitos, ó la cantidad de *feedback* implícito:\n",
    "\n",
    "$$\\hat r_{ui}=\\mu + b_i + b_u + q_{i}'\\biggl(p_{u} + |I_u|^{-1/2} \\sum_{j\\in I_u} y_j \\biggr) $$\n",
    "\n",
    "donde $y_j$ es un valor binario que captura el hecho de que el usuario $u$ haya calificado o revelado su rating para $j$ (sin importar el valor del rating).\n",
    "\n",
    "- NMF\n",
    "\n",
    "Este algoritmo corresponde con la factorización no-negativa de la matriz de ratings, y sigue la misma formulación del SVD. Solo que se garantiza que los factores sean no-negativos.\n",
    "\n",
    "\n",
    "**Algoritmo de clustering:**\n",
    "\n",
    "- Co-clustering\n",
    "\n",
    "En este algoritmo, los usuarios y los items son asignados a los clusters $C_u$, $C_i$ y $C_{ui}$:\n",
    "\n",
    "\n",
    "$$ \\hat r_{ui}=\\bar{C_{ui}} + (\\mu_u - \\bar{C_u}) + (\\mu_i - \\bar{C_i})   $$\n",
    "\n",
    "donde $\\bar{C_{ui}}, \\bar{C_u}, \\bar{C_i}$ son respectivamente los rating promedio de los clusters $C_{ui}, C_u$ y $C_i$.\n",
    "\n",
    "Los clusters se asignan de acuerdo con K-medias.\n",
    "\n",
    "\n",
    "\n",
    "Por defecto se utiliza el RMSE como la métrica de error a minimizar para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "# Implementamos validacion cruzada sobre todos los algoritmos\n",
    "for algoritmo in [SVD(), SVDpp(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    \n",
    "    print(\"\\nAlgoritmo: \", algoritmo)\n",
    "    tiempo = datetime.datetime.now()\n",
    "    print('\\nInicia la validacion cruzada: ', tiempo)\n",
    "    \n",
    "    results = cross_validate(algoritmo, data, measures=['RMSE'], cv=3, verbose=False)\n",
    "    \n",
    "    tiempo = datetime.datetime.now()\n",
    "    print('\\nTermina: ', tiempo)\n",
    "    \n",
    "    # Guardamos los resultados\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algoritmo).split(' ')[0].split('.')[-1]], index=['Algoritmo']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algoritmo').sort_values('test_rmse')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 1\n",
    "\n",
    "Cuál método nos da mejores resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfoquémonos en los modelos que nos dan mejores resultados para entrenar y predecir los ratings de los usuarios. \n",
    "\n",
    "Para su implementación más eficiente, utilizamos la técnica de los Mínimos Cuadrados Alternantes (ALS), en lugar de SGD.\n",
    "\n",
    "### Factorización matricial &  Mínimos Cuadrados Alternantes (ALS) \n",
    "\n",
    "Hasta ahora nos hemos dado cuenta que en Filtrado Colaborativo, la factorización matricial es una técnica clave que permite descomponer la matriz entre usuarios-items en un par de matrices rectangulares de menor dimension.\n",
    "\n",
    "Entonces, una matriz puede ser vista como la matriz de los usuarios, donde cada fila es un usuario y cada columna es una factor latente, y la otra matriz es la de los items, donde cada fila es una factor latente y las columnas representan items. De esta manera, peliculas poco concocidas pueden obtener representaciones latentes enriquecidas, tanto como las películas más conocidas,  lo cual incrementa la capacidad predictiva de los sitemas.\n",
    "\n",
    "En la matriz usuario-item, que es de tipo *sparse* (con muchos ceros en sus entradas), la estimacion de $r_{ui}$  se obtiene como:\n",
    "\n",
    "$$\\hat r_{ui} = \\sum_{f=0}^{|F|} H_{u,f}W_{f,i}$$\n",
    "\n",
    "donde $H$ es la matriz de usuario y $W$ es la matriz de items. \n",
    "\n",
    "Los factores latentes son las nuevas variables o patrones en un espacio de menores dimensiones al de la matriz original (con las observaciones sobre las interacciones usuario-item). \n",
    "\n",
    "El problema de la factorización matricial se puede entender como la minimización de la diferencia entre el rating observado y su estimación:\n",
    "\n",
    "$$ argmin_{H,W} ||R-\\hat R||_F + \\alpha||H|| + \\beta ||W||$$\n",
    "\n",
    "que se puede solucionar por métodos de optimización como el descenso en la dirección del gradiente. Sin embargo, a medida que tenemos un mayor volumen de datos (terabytes y petabytes de datos) necesitamos escalar los algoritmos a procesos en paralelo (paradigma del Big Data).\n",
    "\n",
    "Definamos las opciones de *baselines* para los 3 métodos (https://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baseline-estimates-configuration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 20,\n",
    "               'reg_u': 12, \n",
    "               'reg_i': 5  \n",
    "               }\n",
    "\n",
    "sim_options = {'name': 'cosine',\n",
    "               'user_based': False  # calcula similitudes entre items\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos `train_test_split()` para hacer nuestras particiones de entrenamiento y prueba y usamos la métrica de RMSE. Luego ajustamos el modelo con `fit()` sobre el entrenamiento y lo probamos con `test()` para obtener el desempeño en predicción.\n",
    "\n",
    "La próxima celda tarda unos pocos minutos en correr:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=0.3)\n",
    "\n",
    "SVD = SVDpp(random_state=0)\n",
    "KNN = KNNBaseline(bsl_options=bsl_options, sim_options=sim_options)\n",
    "Base = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "print(\"\\nSVDpp: \")\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nInicia el entrenamiento y prueba: ', tiempo)\n",
    "\n",
    "predSVD = SVD.fit(trainset).test(testset)\n",
    "print(\"RMSE del SVDpp: \", accuracy.rmse(predSVD))\n",
    "\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nTermina: ', tiempo)\n",
    "\n",
    "print(\"\\nKNNBaseline: \")\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nInicia el entrenamiento y prueba: ', tiempo)\n",
    "\n",
    "predKNN = KNN.fit(trainset).test(testset)\n",
    "print(\"\\nRMSE del KNNBaseline: \", accuracy.rmse(predKNN))\n",
    "\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nTermina: ', tiempo)\n",
    "\n",
    "print(\"\\nBaselineOnly: \")\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nInicia el entrenamiento y prueba: ', tiempo)\n",
    "\n",
    "predBase = Base.fit(trainset).test(testset)\n",
    "print(\"\\nRMSE del BaselineOnly: \", accuracy.rmse(predBase))\n",
    "\n",
    "tiempo = datetime.datetime.now()\n",
    "print('\\nTermina: ', tiempo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionemos las predicciones, para ello utilicemos las siguientes funciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def items(uid):\n",
    "    \"\"\" \n",
    "    Input:\n",
    "    uid: el id del usuario\n",
    "    Output: \n",
    "    items calificados por el usuario\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return len(trainset.ur[trainset.to_inner_uid(uid)])\n",
    "    except ValueError: # no se encuentra el usuario\n",
    "        return 0\n",
    "    \n",
    "def usuarios(iid):\n",
    "    \"\"\" \n",
    "    Input: \n",
    "    iid: el id del item\n",
    "    Output:\n",
    "    usuarios que han calificado el item.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        return len(trainset.ir[trainset.to_inner_iid(iid)])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "df = pd.DataFrame(predKNN, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
    "df['Num_Items'] = df.uid.apply(items)\n",
    "df['Num_Usuarios'] = df.iid.apply(usuarios)\n",
    "df['Error'] = abs(df.est - df.rui)\n",
    "\n",
    "mejores = df.sort_values(by='Error')[:10]\n",
    "peores = df.sort_values(by='Error')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mejores[['Num_Items','Num_Usuarios','uid','iid','rui','est','Error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Se tiene una pelicula con muchas calificaciones: \", \n",
    "      np.squeeze(mejores[mejores.iid == 318]['Num_Usuarios']),\n",
    "      np.squeeze(pelis[pelis.id == '318']['original_title']))\n",
    "\n",
    "print(\"O una pelicula con menos calificaciones: \", \n",
    "      np.squeeze(mejores[mejores.iid == 608]['Num_Usuarios']),\n",
    "      np.squeeze(pelis[pelis.id == '608']['original_title']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos las peores predicciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peores[['Num_Items','Num_Usuarios','uid','iid','rui','est','Error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"También se tiene una pelicula con muchas calificaciones: \", \n",
    "      np.squeeze(peores[peores.iid == 2959]['Num_Usuarios']),\n",
    "      np.squeeze(pelis[pelis.id == '2959']['original_title']))\n",
    "\n",
    "print(\"O una pelicula con menos calificaciones: \", \n",
    "      np.squeeze(peores[peores.iid == 3160]['Num_Usuarios']),\n",
    "      np.squeeze(pelis[pelis.id == '3160']['original_title']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinando las peores predicciones, los errores parecen bastante altos. Veamos la ultima pelicula \"Furankenshutain Tai Chitei Kaijū Baragon\", que fue calificada por 34 usuarios, donde el usuario \"83\" calificó 98 películas.\n",
    "\n",
    "Nuestro algoritmo del KNNBaseline predijo que este usuario calificaría esta peliucla con un 4.22, oero el usuario le dió un 0.5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "df_nuevo.loc[df_nuevo['movieId'] == 2959]['rating'].hist()\n",
    "plt.xlabel('rating')\n",
    "plt.ylabel('Numero de ratings')\n",
    "plt.title('Numero de ratings de License to Wed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que esta pelicula recibió mayormente ratings cercanos a 4.0, donde la mayoría de los usuarios de nuestros datos de entrenamiento calificaron la pelicula como buena en lugar de darle una mala calificación. \n",
    "\n",
    "Al tratarse de recomendaciones personalizadas, puede que las peores recomendaciones contengan los usuarios más raros o con gustos más peculiares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluación de sistemas y métricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta ahora hemos utilizado el RMSE para evaluar nuestros modelos, pero también podemos ver qué ocurre si usamos una medida distinta, como por ejemplo el índice FCP (*Fraction Concordant Pairs*), propuesto por (Koren & Sill 2011). Esta métrica no se concentra en el rating y su valor cardinal, sino más bien en su valor ordinal, de tal manera que captura cuándo un item es asignado una posición en el ranking de preferencia al menos tan buena como la que le asignó el usuario.\n",
    "\n",
    "EL FCP está definido por \n",
    "\n",
    "$$ FCP=\\frac{n_c}{n_c+n_d}$$\n",
    "\n",
    "donde el número de items concordantes $n_c=\\sum_u n_c^u$ se calcula a partir de \n",
    "\n",
    "$$ n_c^u = \\{(i,j)|\\hat r_{ui} > \\hat r_{uj} \\text{ y } r_{ui}>r_{uj}\\} $$\n",
    "\n",
    "y el número de items discordantes $n_d^u$ para el usuario $u$ se calcular de manera similar.\n",
    "\n",
    "De hecho esta medida geenraliza el AUC para respuestas ordinales categóricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2\n",
    "\n",
    "Encuentre la metrica FCP de los modelos previamente entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pregunta 3\n",
    "\n",
    "Cuál modelo elegiríamos basados en la FCP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Intente mejorar los resultados del algoritmo SVD optimizando la métrica del FCP sobre los términos de regularización o el método de optimizacion (SGD o ALS).\n",
    "\n",
    "*Ayuda: puede explorar https://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "\n",
    "De manera análoga al ejercicio anterior, intente mejorar los resultados del algoritmo de KNN optimizando la métrica del FCP sobre el numero de vecinos o la medida de similitud utilizada.\n",
    "\n",
    "*Ayuda: puede explorar https://surprise.readthedocs.io/en/stable/knn_inspired.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
